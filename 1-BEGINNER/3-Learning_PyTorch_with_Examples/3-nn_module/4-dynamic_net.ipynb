{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: Control Flow + Weight Sharing\n",
    "--------------------------------------\n",
    "\n",
    "To showcase the power of `PyTorch dynamic graphs`, \n",
    "\n",
    "we will implement a very strange model: \n",
    "\n",
    "a fully-connected ReLU network that on each forward pass randomly chooses\n",
    "a number between 1 and 4 \n",
    "\n",
    "and has that many hidden layers, \n",
    "\n",
    "reusing the same weights multiple times to compute the innermost hidden layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we construct three nn.Linear instances that we will use\n",
    "        in the forward pass.\n",
    "        \"\"\"\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_linear  = torch.nn.Linear(D_in, H)\n",
    "        self.middle_linear = torch.nn.Linear(H, H)\n",
    "        self.output_linear = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        For the forward pass of the model, \n",
    "        we randomly choose either 0, 1, 2, or 3\n",
    "        and reuse the middle_linear Module that many times to compute hidden layer\n",
    "        representations.\n",
    "\n",
    "        Since each forward pass builds a dynamic computation graph, we can use normal\n",
    "        Python control-flow operators like loops or conditional statements when\n",
    "        defining the forward pass of the model.\n",
    "\n",
    "        Here we also see that it is perfectly safe to reuse the same Module many\n",
    "        times when defining a computational graph. This is a big improvement from Lua\n",
    "        Torch, where each Module could be used only once.\n",
    "        \"\"\"\n",
    "        h_relu = self.input_linear(x).clamp(min=0)\n",
    "        # Multiple Middle_Linear 0 times, or 1, or 3 times\n",
    "        for _ in range(random.randint(0, 3)):\n",
    "            h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
    "        y_pred = self.output_linear(h_relu)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0531384944915771\n",
      "1 1.0986714363098145\n",
      "2 1.053126335144043\n",
      "3 1.0520378351211548\n",
      "4 1.0520329475402832\n",
      "5 1.0618231296539307\n",
      "6 1.053080439567566\n",
      "7 1.0530635118484497\n",
      "8 1.0979418754577637\n",
      "9 1.0977392196655273\n",
      "10 1.051985263824463\n",
      "11 1.0616248846054077\n",
      "12 1.0615708827972412\n",
      "13 1.0614978075027466\n",
      "14 1.052924394607544\n",
      "15 1.0529009103775024\n",
      "16 1.0528767108917236\n",
      "17 1.0958902835845947\n",
      "18 1.051896333694458\n",
      "19 1.0518838167190552\n",
      "20 1.0951356887817383\n",
      "21 1.0518563985824585\n",
      "22 1.0608599185943604\n",
      "23 1.094226360321045\n",
      "24 1.0938503742218018\n",
      "25 1.0933845043182373\n",
      "26 1.0517877340316772\n",
      "27 1.0923449993133545\n",
      "28 1.0517594814300537\n",
      "29 1.0912578105926514\n",
      "30 1.0603508949279785\n",
      "31 1.0525166988372803\n",
      "32 1.0517010688781738\n",
      "33 1.0892071723937988\n",
      "34 1.0524446964263916\n",
      "35 1.0600279569625854\n",
      "36 1.0877869129180908\n",
      "37 1.051628828048706\n",
      "38 1.0523440837860107\n",
      "39 1.0523184537887573\n",
      "40 1.051585078239441\n",
      "41 1.0515708923339844\n",
      "42 1.051555871963501\n",
      "43 1.0522117614746094\n",
      "44 1.0848308801651\n",
      "45 1.0515111684799194\n",
      "46 1.0841562747955322\n",
      "47 1.0593724250793457\n",
      "48 1.0514661073684692\n",
      "49 1.0514514446258545\n",
      "50 1.0827155113220215\n",
      "51 1.0823137760162354\n",
      "52 1.0590988397598267\n",
      "53 1.0519721508026123\n",
      "54 1.051946759223938\n",
      "55 1.058902382850647\n",
      "56 1.0588204860687256\n",
      "57 1.0587238073349\n",
      "58 1.0513193607330322\n",
      "59 1.0585081577301025\n",
      "60 1.0517942905426025\n",
      "61 1.0512765645980835\n",
      "62 1.0788419246673584\n",
      "63 1.0785473585128784\n",
      "64 1.0516942739486694\n",
      "65 1.0516676902770996\n",
      "66 1.0512049198150635\n",
      "67 1.0516117811203003\n",
      "68 1.0515830516815186\n",
      "69 1.0767041444778442\n",
      "70 1.0763672590255737\n",
      "71 1.051134705543518\n",
      "72 1.0514639616012573\n",
      "73 1.0511066913604736\n",
      "74 1.0510928630828857\n",
      "75 1.051077961921692\n",
      "76 1.051356315612793\n",
      "77 1.0510488748550415\n",
      "78 1.0572491884231567\n",
      "79 1.0571935176849365\n",
      "80 1.0571177005767822\n",
      "81 1.051228642463684\n",
      "82 1.0732117891311646\n",
      "83 1.056858777999878\n",
      "84 1.051151156425476\n",
      "85 1.0725034475326538\n",
      "86 1.0509215593338013\n",
      "87 1.050907850265503\n",
      "88 1.050893783569336\n",
      "89 1.0714099407196045\n",
      "90 1.0563085079193115\n",
      "91 1.0562280416488647\n",
      "92 1.0508356094360352\n",
      "93 1.0560410022735596\n",
      "94 1.0508064031600952\n",
      "95 1.0508921146392822\n",
      "96 1.0507770776748657\n",
      "97 1.0556702613830566\n",
      "98 1.0507481098175049\n",
      "99 1.0508034229278564\n",
      "100 1.0690265893936157\n",
      "101 1.050706148147583\n",
      "102 1.0507348775863647\n",
      "103 1.0506774187088013\n",
      "104 1.0506620407104492\n",
      "105 1.0680317878723145\n",
      "106 1.0677658319473267\n",
      "107 1.0506188869476318\n",
      "108 1.050604224205017\n",
      "109 1.0667812824249268\n",
      "110 1.0663949251174927\n",
      "111 1.0547785758972168\n",
      "112 1.0547133684158325\n",
      "113 1.0651159286499023\n",
      "114 1.0504571199417114\n",
      "115 1.0505050420761108\n",
      "116 1.050408124923706\n",
      "117 1.0543537139892578\n",
      "118 1.0542737245559692\n",
      "119 1.0504496097564697\n",
      "120 1.050435185432434\n",
      "121 1.0502865314483643\n",
      "122 1.0502625703811646\n",
      "123 1.061995267868042\n",
      "124 1.0537984371185303\n",
      "125 1.0614333152770996\n",
      "126 1.0610721111297607\n",
      "127 1.0606273412704468\n",
      "128 1.0601087808609009\n",
      "129 1.0534226894378662\n",
      "130 1.0500597953796387\n",
      "131 1.0502843856811523\n",
      "132 1.05027174949646\n",
      "133 1.0531213283538818\n",
      "134 1.0502439737319946\n",
      "135 1.052959680557251\n",
      "136 1.0499202013015747\n",
      "137 1.0527805089950562\n",
      "138 1.0501883029937744\n",
      "139 1.0559985637664795\n",
      "140 1.0498292446136475\n",
      "141 1.050147533416748\n",
      "142 1.0551408529281616\n",
      "143 1.0522725582122803\n",
      "144 1.0521851778030396\n",
      "145 1.0520827770233154\n",
      "146 1.0519665479660034\n",
      "147 1.0496680736541748\n",
      "148 1.0517184734344482\n",
      "149 1.0532582998275757\n",
      "150 1.0514681339263916\n",
      "151 1.0513360500335693\n",
      "152 1.051192283630371\n",
      "153 1.0510401725769043\n",
      "154 1.0520089864730835\n",
      "155 1.0499603748321533\n",
      "156 1.0499460697174072\n",
      "157 1.0511837005615234\n",
      "158 1.0508432388305664\n",
      "159 1.0504199266433716\n",
      "160 1.0499221086502075\n",
      "161 1.050065040588379\n",
      "162 1.0499589443206787\n",
      "163 1.0498398542404175\n",
      "164 1.0497084856033325\n",
      "165 1.049567461013794\n",
      "166 1.049416422843933\n",
      "167 1.0492568016052246\n",
      "168 1.0466392040252686\n",
      "169 1.0462663173675537\n",
      "170 1.049759864807129\n",
      "171 1.049747109413147\n",
      "172 1.0491527318954468\n",
      "173 1.049720048904419\n",
      "174 1.0491127967834473\n",
      "175 1.0482556819915771\n",
      "176 1.0438880920410156\n",
      "177 1.0490458011627197\n",
      "178 1.0432440042495728\n",
      "179 1.0428531169891357\n",
      "180 1.0489763021469116\n",
      "181 1.0477341413497925\n",
      "182 1.048925757408142\n",
      "183 1.0475654602050781\n",
      "184 1.0409252643585205\n",
      "185 1.0488489866256714\n",
      "186 1.0495506525039673\n",
      "187 1.0398461818695068\n",
      "188 1.0394405126571655\n",
      "189 1.0495116710662842\n",
      "190 1.0494974851608276\n",
      "191 1.046967625617981\n",
      "192 1.0377843379974365\n",
      "193 1.0494587421417236\n",
      "194 1.0494450330734253\n",
      "195 1.048622488975525\n",
      "196 1.0362917184829712\n",
      "197 1.0485800504684448\n",
      "198 1.0485564470291138\n",
      "199 1.0352039337158203\n",
      "200 1.0347952842712402\n",
      "201 1.0464187860488892\n",
      "202 1.0463570356369019\n",
      "203 1.0493276119232178\n",
      "204 1.0493136644363403\n",
      "205 1.0461418628692627\n",
      "206 1.0460593700408936\n",
      "207 1.048351764678955\n",
      "208 1.0320169925689697\n",
      "209 1.0457875728607178\n",
      "210 1.0482869148254395\n",
      "211 1.0482633113861084\n",
      "212 1.0308754444122314\n",
      "213 1.0482113361358643\n",
      "214 1.0302424430847168\n",
      "215 1.0491735935211182\n",
      "216 1.0481337308883667\n",
      "217 1.0292013883590698\n",
      "218 1.0451291799545288\n",
      "219 1.0491230487823486\n",
      "220 1.044991135597229\n",
      "221 1.044909954071045\n",
      "222 1.0479875802993774\n",
      "223 1.044722557067871\n",
      "224 1.049060583114624\n",
      "225 1.0479166507720947\n",
      "226 1.0478918552398682\n",
      "227 1.0478650331497192\n",
      "228 1.0490113496780396\n",
      "229 1.04780912399292\n",
      "230 1.0489871501922607\n",
      "231 1.0260493755340576\n",
      "232 1.0258370637893677\n",
      "233 1.0255348682403564\n",
      "234 1.025151014328003\n",
      "235 1.047655701637268\n",
      "236 1.024282693862915\n",
      "237 1.0438135862350464\n",
      "238 1.0233657360076904\n",
      "239 1.0228642225265503\n",
      "240 1.0436433553695679\n",
      "241 1.0475142002105713\n",
      "242 1.043505072593689\n",
      "243 1.0209230184555054\n",
      "244 1.0433454513549805\n",
      "245 1.048799991607666\n",
      "246 1.0431691408157349\n",
      "247 1.0473783016204834\n",
      "248 1.0473549365997314\n",
      "249 1.0473302602767944\n",
      "250 1.0473014116287231\n",
      "251 1.0472736358642578\n",
      "252 1.0487157106399536\n",
      "253 1.0426247119903564\n",
      "254 1.0486912727355957\n",
      "255 1.0471595525741577\n",
      "256 1.0471323728561401\n",
      "257 1.0486558675765991\n",
      "258 1.0422972440719604\n",
      "259 1.0470483303070068\n",
      "260 1.0470194816589355\n",
      "261 1.0486071109771729\n",
      "262 1.0469619035720825\n",
      "263 1.0165009498596191\n",
      "264 1.0163214206695557\n",
      "265 1.0468764305114746\n",
      "266 1.0158058404922485\n",
      "267 1.0485355854034424\n",
      "268 1.0417629480361938\n",
      "269 1.0417046546936035\n",
      "270 1.041629433631897\n",
      "271 1.0415399074554443\n",
      "272 1.0466983318328857\n",
      "273 1.0466731786727905\n",
      "274 1.0484509468078613\n",
      "275 1.013739824295044\n",
      "276 1.0484269857406616\n",
      "277 1.0410311222076416\n",
      "278 1.0484024286270142\n",
      "279 1.0483906269073486\n",
      "280 1.0408003330230713\n",
      "281 1.0464893579483032\n",
      "282 1.0406349897384644\n",
      "283 1.012312650680542\n",
      "284 1.0464246273040771\n",
      "285 1.0483165979385376\n",
      "286 1.0403029918670654\n",
      "287 1.0402135848999023\n",
      "288 1.0482795238494873\n",
      "289 1.0112569332122803\n",
      "290 1.0399327278137207\n",
      "291 1.046278715133667\n",
      "292 1.0397412776947021\n",
      "293 1.0396356582641602\n",
      "294 1.0482056140899658\n",
      "295 1.0461945533752441\n",
      "296 1.039314866065979\n",
      "297 1.0481693744659424\n",
      "298 1.039103388786316\n",
      "299 1.009687066078186\n",
      "300 1.0481343269348145\n",
      "301 1.0481215715408325\n",
      "302 1.0387060642242432\n",
      "303 1.0480966567993164\n",
      "304 1.0385147333145142\n",
      "305 1.0480711460113525\n",
      "306 1.0459877252578735\n",
      "307 1.0480457544326782\n",
      "308 1.0480331182479858\n",
      "309 1.038076400756836\n",
      "310 1.048007607460022\n",
      "311 1.0479943752288818\n",
      "312 1.0458840131759644\n",
      "313 1.0458658933639526\n",
      "314 1.0080159902572632\n",
      "315 1.0078579187393188\n",
      "316 1.0479304790496826\n",
      "317 1.0073840618133545\n",
      "318 1.0479047298431396\n",
      "319 1.0457475185394287\n",
      "320 1.0374104976654053\n",
      "321 1.0457067489624023\n",
      "322 1.006105899810791\n",
      "323 1.0478435754776\n",
      "324 1.0456422567367554\n",
      "325 1.0478190183639526\n",
      "326 1.0455968379974365\n",
      "327 1.0370745658874512\n",
      "328 1.0477831363677979\n",
      "329 1.047770619392395\n",
      "330 1.0044033527374268\n",
      "331 1.0454868078231812\n",
      "332 1.0039494037628174\n",
      "333 1.0367895364761353\n",
      "334 1.0367319583892822\n",
      "335 1.0031304359436035\n",
      "336 1.0028038024902344\n",
      "337 1.045361042022705\n",
      "338 1.0453392267227173\n",
      "339 1.0364105701446533\n",
      "340 1.0014193058013916\n",
      "341 1.0010480880737305\n",
      "342 1.0362107753753662\n",
      "343 1.0452258586883545\n",
      "344 1.0475965738296509\n",
      "345 0.9995274543762207\n",
      "346 1.0475746393203735\n",
      "347 1.0475618839263916\n",
      "348 1.0451200008392334\n",
      "349 0.998157799243927\n",
      "350 1.035738229751587\n",
      "351 1.0475146770477295\n",
      "352 1.0450382232666016\n",
      "353 1.0474917888641357\n",
      "354 1.0474793910980225\n",
      "355 0.9964176416397095\n",
      "356 0.9961124658584595\n",
      "357 1.0474432706832886\n",
      "358 1.0449200868606567\n",
      "359 1.047417402267456\n",
      "360 1.0474058389663696\n",
      "361 0.9945549964904785\n",
      "362 0.9942238926887512\n",
      "363 1.0352084636688232\n",
      "364 1.0473560094833374\n",
      "365 1.0447914600372314\n",
      "366 1.0447719097137451\n",
      "367 1.0473201274871826\n",
      "368 0.9923259615898132\n",
      "369 0.9920046925544739\n",
      "370 1.0472835302352905\n",
      "371 1.0349175930023193\n",
      "372 1.044653296470642\n",
      "373 0.9906466603279114\n",
      "374 1.0347844362258911\n",
      "375 1.047224760055542\n",
      "376 1.0445740222930908\n",
      "377 0.989386260509491\n",
      "378 0.9890419244766235\n",
      "379 0.9886263608932495\n",
      "380 1.0344887971878052\n",
      "381 1.0471559762954712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382 1.0343787670135498\n",
      "383 1.0343071222305298\n",
      "384 1.034220576286316\n",
      "385 1.0444053411483765\n",
      "386 1.0443860292434692\n",
      "387 1.033950686454773\n",
      "388 1.044343113899231\n",
      "389 1.0443191528320312\n",
      "390 1.0336850881576538\n",
      "391 1.044269323348999\n",
      "392 1.0442438125610352\n",
      "393 1.0334243774414062\n",
      "394 0.9847644567489624\n",
      "395 1.044164776802063\n",
      "396 0.9843646883964539\n",
      "397 1.0441110134124756\n",
      "398 1.0469725131988525\n",
      "399 1.0440572500228882\n",
      "400 1.0440301895141602\n",
      "401 1.0440013408660889\n",
      "402 1.0328171253204346\n",
      "403 0.982917308807373\n",
      "404 1.0439143180847168\n",
      "405 1.0326379537582397\n",
      "406 1.0325672626495361\n",
      "407 1.0324804782867432\n",
      "408 1.0323821306228638\n",
      "409 1.0468568801879883\n",
      "410 1.04376220703125\n",
      "411 1.0437383651733398\n",
      "412 0.9814281463623047\n",
      "413 0.9812291264533997\n",
      "414 1.0318472385406494\n",
      "415 1.0436410903930664\n",
      "416 0.980462908744812\n",
      "417 0.9801542162895203\n",
      "418 0.9797749519348145\n",
      "419 1.0314886569976807\n",
      "420 1.04674232006073\n",
      "421 1.0435093641281128\n",
      "422 0.9782465696334839\n",
      "423 1.0434656143188477\n",
      "424 1.031169056892395\n",
      "425 0.9771747589111328\n",
      "426 0.9767860174179077\n",
      "427 1.0466703176498413\n",
      "428 1.0466594696044922\n",
      "429 0.9755582809448242\n",
      "430 1.0466383695602417\n",
      "431 0.9747387170791626\n",
      "432 1.0466158390045166\n",
      "433 1.0466039180755615\n",
      "434 1.0306785106658936\n",
      "435 1.0465806722640991\n",
      "436 1.0465691089630127\n",
      "437 1.0432138442993164\n",
      "438 1.0304954051971436\n",
      "439 1.0304369926452637\n",
      "440 1.0303627252578735\n",
      "441 0.9717726707458496\n",
      "442 1.030193567276001\n",
      "443 1.0431163311004639\n",
      "444 0.9710592031478882\n",
      "445 1.0430797338485718\n",
      "446 1.0298616886138916\n",
      "447 1.0297752618789673\n",
      "448 1.0464365482330322\n",
      "449 1.0295844078063965\n",
      "450 0.9696733355522156\n",
      "451 1.0464050769805908\n",
      "452 1.0463945865631104\n",
      "453 1.0429364442825317\n",
      "454 0.9687795639038086\n",
      "455 1.046360969543457\n",
      "456 0.9682632684707642\n",
      "457 0.9679428339004517\n",
      "458 1.042848825454712\n",
      "459 1.0288805961608887\n",
      "460 0.9668922424316406\n",
      "461 1.0427944660186768\n",
      "462 1.046285629272461\n",
      "463 1.04627525806427\n",
      "464 1.046264886856079\n",
      "465 1.046251654624939\n",
      "466 1.0285351276397705\n",
      "467 1.042685627937317\n",
      "468 1.0284267663955688\n",
      "469 1.0462075471878052\n",
      "470 0.9643662571907043\n",
      "471 0.9641299247741699\n",
      "472 1.0461745262145996\n",
      "473 1.0461629629135132\n",
      "474 1.0280977487564087\n",
      "475 0.9630526304244995\n",
      "476 1.0279815196990967\n",
      "477 1.0461184978485107\n",
      "478 1.042508602142334\n",
      "479 1.027783989906311\n",
      "480 1.0277090072631836\n",
      "481 0.9616141319274902\n",
      "482 1.0275377035140991\n",
      "483 1.0274426937103271\n",
      "484 1.0273363590240479\n",
      "485 0.9607052803039551\n",
      "486 1.046022653579712\n",
      "487 1.0423665046691895\n",
      "488 0.9599648714065552\n",
      "489 0.9596664309501648\n",
      "490 1.0423157215118408\n",
      "491 1.0459710359573364\n",
      "492 1.0459612607955933\n",
      "493 1.0459508895874023\n",
      "494 1.0422438383102417\n",
      "495 1.0422255992889404\n",
      "496 1.0459179878234863\n",
      "497 1.0459057092666626\n",
      "498 1.0421640872955322\n",
      "499 0.9572504162788391\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = DynamicNet(D_in, H, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. Training this strange model with\n",
    "# vanilla stochastic gradient descent is tough, so we use momentum\n",
    "\n",
    "criterion = torch.nn.MSELoss() # reduction='sum'\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "for t in range(500):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
