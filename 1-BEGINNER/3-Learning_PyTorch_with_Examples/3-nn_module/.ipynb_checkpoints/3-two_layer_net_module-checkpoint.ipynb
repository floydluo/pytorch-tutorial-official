{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: Custom nn Modules\n",
    "--------------------------\n",
    "\n",
    "A fully-connected ReLU network with one hidden layer, trained to predict y from x\n",
    "by minimizing squared Euclidean distance.\n",
    "\n",
    "This implementation defines the model as a custom Module subclass. Whenever you\n",
    "want a model more complex than a simple sequence of existing Modules you will\n",
    "need to define your model this way.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# construction a Computational Graph\n",
    "# Take D_in, H, D_out as the inputs\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor \n",
    "        we instantiate two nn.Linear modules \n",
    "        and assign them as member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data \n",
    "        and we must return a Tensor of output data. \n",
    "        We can use Modules defined in the constructor \n",
    "        as well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TwoLayerNet(\n",
       "  (linear1): Linear(in_features=1000, out_features=100, bias=True)\n",
       "  (linear2): Linear(in_features=100, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct our model by instantiating the class defined above\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "model\n",
    "# notice, here is a bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0750528573989868\n",
      "1 1.0749151706695557\n",
      "2 1.0747783184051514\n",
      "3 1.074641227722168\n",
      "4 1.0745043754577637\n",
      "5 1.0743672847747803\n",
      "6 1.0742309093475342\n",
      "7 1.0740950107574463\n",
      "8 1.0739580392837524\n",
      "9 1.0738216638565063\n",
      "10 1.073686122894287\n",
      "11 1.0735502243041992\n",
      "12 1.0734131336212158\n",
      "13 1.0732777118682861\n",
      "14 1.0731408596038818\n",
      "15 1.0730042457580566\n",
      "16 1.0728687047958374\n",
      "17 1.072731375694275\n",
      "18 1.0725959539413452\n",
      "19 1.0724600553512573\n",
      "20 1.0723241567611694\n",
      "21 1.0721882581710815\n",
      "22 1.0720518827438354\n",
      "23 1.0719163417816162\n",
      "24 1.0717798471450806\n",
      "25 1.0716444253921509\n",
      "26 1.0715082883834839\n",
      "27 1.0713728666305542\n",
      "28 1.0712363719940186\n",
      "29 1.071101427078247\n",
      "30 1.070965051651001\n",
      "31 1.070828914642334\n",
      "32 1.070693850517273\n",
      "33 1.0705584287643433\n",
      "34 1.0704232454299927\n",
      "35 1.0702866315841675\n",
      "36 1.0701513290405273\n",
      "37 1.0700159072875977\n",
      "38 1.0698808431625366\n",
      "39 1.069745421409607\n",
      "40 1.0696094036102295\n",
      "41 1.069474458694458\n",
      "42 1.069339394569397\n",
      "43 1.0692040920257568\n",
      "44 1.0690690279006958\n",
      "45 1.0689338445663452\n",
      "46 1.0687988996505737\n",
      "47 1.068663477897644\n",
      "48 1.0685279369354248\n",
      "49 1.0683928728103638\n",
      "50 1.0682575702667236\n",
      "51 1.0681222677230835\n",
      "52 1.0679876804351807\n",
      "53 1.0678534507751465\n",
      "54 1.067718267440796\n",
      "55 1.0675830841064453\n",
      "56 1.0674481391906738\n",
      "57 1.06731379032135\n",
      "58 1.0671788454055786\n",
      "59 1.0670440196990967\n",
      "60 1.0669091939926147\n",
      "61 1.0667741298675537\n",
      "62 1.06663978099823\n",
      "63 1.0665055513381958\n",
      "64 1.0663702487945557\n",
      "65 1.0662367343902588\n",
      "66 1.0661019086837769\n",
      "67 1.0659677982330322\n",
      "68 1.0658329725265503\n",
      "69 1.0656980276107788\n",
      "70 1.0655642747879028\n",
      "71 1.0654287338256836\n",
      "72 1.0652954578399658\n",
      "73 1.0651609897613525\n",
      "74 1.0650265216827393\n",
      "75 1.0648919343948364\n",
      "76 1.0647578239440918\n",
      "77 1.064623236656189\n",
      "78 1.0644899606704712\n",
      "79 1.0643552541732788\n",
      "80 1.0642218589782715\n",
      "81 1.064086675643921\n",
      "82 1.063953161239624\n",
      "83 1.063819169998169\n",
      "84 1.0636847019195557\n",
      "85 1.063551425933838\n",
      "86 1.063417673110962\n",
      "87 1.0632842779159546\n",
      "88 1.0631498098373413\n",
      "89 1.0630172491073608\n",
      "90 1.0628831386566162\n",
      "91 1.0627501010894775\n",
      "92 1.0626165866851807\n",
      "93 1.0624821186065674\n",
      "94 1.0623492002487183\n",
      "95 1.0622155666351318\n",
      "96 1.0620818138122559\n",
      "97 1.0619488954544067\n",
      "98 1.0618150234222412\n",
      "99 1.061682105064392\n",
      "100 1.0615482330322266\n",
      "101 1.0614149570465088\n",
      "102 1.0612813234329224\n",
      "103 1.0611485242843628\n",
      "104 1.0610148906707764\n",
      "105 1.0608822107315063\n",
      "106 1.0607497692108154\n",
      "107 1.0606155395507812\n",
      "108 1.0604833364486694\n",
      "109 1.060351014137268\n",
      "110 1.0602171421051025\n",
      "111 1.0600855350494385\n",
      "112 1.059952735900879\n",
      "113 1.0598210096359253\n",
      "114 1.059687614440918\n",
      "115 1.059556007385254\n",
      "116 1.0594227313995361\n",
      "117 1.0592905282974243\n",
      "118 1.0591576099395752\n",
      "119 1.0590250492095947\n",
      "120 1.0588940382003784\n",
      "121 1.0587612390518188\n",
      "122 1.0586283206939697\n",
      "123 1.0584967136383057\n",
      "124 1.0583646297454834\n",
      "125 1.0582321882247925\n",
      "126 1.0580995082855225\n",
      "127 1.057966947555542\n",
      "128 1.0578362941741943\n",
      "129 1.0577030181884766\n",
      "130 1.0575717687606812\n",
      "131 1.0574394464492798\n",
      "132 1.0573071241378784\n",
      "133 1.0571744441986084\n",
      "134 1.0570436716079712\n",
      "135 1.0569108724594116\n",
      "136 1.056778907775879\n",
      "137 1.0566478967666626\n",
      "138 1.0565154552459717\n",
      "139 1.0563842058181763\n",
      "140 1.056252121925354\n",
      "141 1.056119680404663\n",
      "142 1.0559886693954468\n",
      "143 1.0558563470840454\n",
      "144 1.0557254552841187\n",
      "145 1.055593729019165\n",
      "146 1.0554618835449219\n",
      "147 1.0553302764892578\n",
      "148 1.0551984310150146\n",
      "149 1.0550673007965088\n",
      "150 1.054936170578003\n",
      "151 1.0548040866851807\n",
      "152 1.054673433303833\n",
      "153 1.054541826248169\n",
      "154 1.0544105768203735\n",
      "155 1.0542795658111572\n",
      "156 1.0541472434997559\n",
      "157 1.0540159940719604\n",
      "158 1.0538856983184814\n",
      "159 1.0537536144256592\n",
      "160 1.0536223649978638\n",
      "161 1.053491473197937\n",
      "162 1.0533604621887207\n",
      "163 1.053229570388794\n",
      "164 1.0530987977981567\n",
      "165 1.0529677867889404\n",
      "166 1.0528364181518555\n",
      "167 1.0527050495147705\n",
      "168 1.052574634552002\n",
      "169 1.0524437427520752\n",
      "170 1.052312970161438\n",
      "171 1.0521814823150635\n",
      "172 1.0520511865615845\n",
      "173 1.0519202947616577\n",
      "174 1.051790475845337\n",
      "175 1.051659345626831\n",
      "176 1.0515278577804565\n",
      "177 1.0513980388641357\n",
      "178 1.0512664318084717\n",
      "179 1.0511367321014404\n",
      "180 1.0510053634643555\n",
      "181 1.0508750677108765\n",
      "182 1.0507447719573975\n",
      "183 1.050614595413208\n",
      "184 1.0504838228225708\n",
      "185 1.0503536462783813\n",
      "186 1.05022394657135\n",
      "187 1.050093412399292\n",
      "188 1.0499637126922607\n",
      "189 1.049833059310913\n",
      "190 1.0497033596038818\n",
      "191 1.0495736598968506\n",
      "192 1.0494434833526611\n",
      "193 1.0493130683898926\n",
      "194 1.0491831302642822\n",
      "195 1.0490533113479614\n",
      "196 1.0489238500595093\n",
      "197 1.0487936735153198\n",
      "198 1.0486646890640259\n",
      "199 1.0485340356826782\n",
      "200 1.0484039783477783\n",
      "201 1.0482746362686157\n",
      "202 1.0481449365615845\n",
      "203 1.0480155944824219\n",
      "204 1.0478864908218384\n",
      "205 1.0477569103240967\n",
      "206 1.0476276874542236\n",
      "207 1.0474978685379028\n",
      "208 1.0473686456680298\n",
      "209 1.0472391843795776\n",
      "210 1.0471093654632568\n",
      "211 1.0469796657562256\n",
      "212 1.0468494892120361\n",
      "213 1.0467219352722168\n",
      "214 1.0465924739837646\n",
      "215 1.0464626550674438\n",
      "216 1.0463337898254395\n",
      "217 1.0462055206298828\n",
      "218 1.046076774597168\n",
      "219 1.045947790145874\n",
      "220 1.0458180904388428\n",
      "221 1.0456899404525757\n",
      "222 1.045561671257019\n",
      "223 1.0454323291778564\n",
      "224 1.045303225517273\n",
      "225 1.045174479484558\n",
      "226 1.0450456142425537\n",
      "227 1.0449172258377075\n",
      "228 1.044788122177124\n",
      "229 1.0446594953536987\n",
      "230 1.0445306301116943\n",
      "231 1.044403076171875\n",
      "232 1.044274091720581\n",
      "233 1.0441458225250244\n",
      "234 1.0440179109573364\n",
      "235 1.0438892841339111\n",
      "236 1.0437606573104858\n",
      "237 1.0436323881149292\n",
      "238 1.0435045957565308\n",
      "239 1.043376088142395\n",
      "240 1.0432476997375488\n",
      "241 1.0431190729141235\n",
      "242 1.042991280555725\n",
      "243 1.042862892150879\n",
      "244 1.0427348613739014\n",
      "245 1.042607069015503\n",
      "246 1.0424792766571045\n",
      "247 1.0423505306243896\n",
      "248 1.0422227382659912\n",
      "249 1.0420951843261719\n",
      "250 1.041967511177063\n",
      "251 1.041839599609375\n",
      "252 1.0417119264602661\n",
      "253 1.0415840148925781\n",
      "254 1.0414564609527588\n",
      "255 1.04132878780365\n",
      "256 1.0412006378173828\n",
      "257 1.041073203086853\n",
      "258 1.0409458875656128\n",
      "259 1.040818214416504\n",
      "260 1.0406904220581055\n",
      "261 1.0405627489089966\n",
      "262 1.040435552597046\n",
      "263 1.0403072834014893\n",
      "264 1.04017972946167\n",
      "265 1.040052056312561\n",
      "266 1.0399245023727417\n",
      "267 1.0397981405258179\n",
      "268 1.0396702289581299\n",
      "269 1.0395429134368896\n",
      "270 1.039414882659912\n",
      "271 1.0392881631851196\n",
      "272 1.0391600131988525\n",
      "273 1.0390331745147705\n",
      "274 1.0389055013656616\n",
      "275 1.0387778282165527\n",
      "276 1.0386520624160767\n",
      "277 1.03852379322052\n",
      "278 1.0383974313735962\n",
      "279 1.0382699966430664\n",
      "280 1.0381429195404053\n",
      "281 1.0380158424377441\n",
      "282 1.037888526916504\n",
      "283 1.0377609729766846\n",
      "284 1.0376338958740234\n",
      "285 1.0375069379806519\n",
      "286 1.0373804569244385\n",
      "287 1.0372530221939087\n",
      "288 1.037126064300537\n",
      "289 1.0369988679885864\n",
      "290 1.0368726253509521\n",
      "291 1.0367449522018433\n",
      "292 1.0366181135177612\n",
      "293 1.036491870880127\n",
      "294 1.0363649129867554\n",
      "295 1.0362389087677002\n",
      "296 1.0361125469207764\n",
      "297 1.0359859466552734\n",
      "298 1.035859227180481\n",
      "299 1.0357329845428467\n",
      "300 1.0356067419052124\n",
      "301 1.0354799032211304\n",
      "302 1.0353529453277588\n",
      "303 1.035226821899414\n",
      "304 1.0351006984710693\n",
      "305 1.0349740982055664\n",
      "306 1.0348477363586426\n",
      "307 1.0347216129302979\n",
      "308 1.0345960855484009\n",
      "309 1.0344693660736084\n",
      "310 1.0343436002731323\n",
      "311 1.0342170000076294\n",
      "312 1.0340906381607056\n",
      "313 1.0339652299880981\n",
      "314 1.0338389873504639\n",
      "315 1.0337135791778564\n",
      "316 1.0335874557495117\n",
      "317 1.0334616899490356\n",
      "318 1.0333354473114014\n",
      "319 1.0332098007202148\n",
      "320 1.0330841541290283\n",
      "321 1.0329580307006836\n",
      "322 1.032832384109497\n",
      "323 1.0327069759368896\n",
      "324 1.0325814485549927\n",
      "325 1.0324560403823853\n",
      "326 1.0323303937911987\n",
      "327 1.0322051048278809\n",
      "328 1.0320794582366943\n",
      "329 1.0319539308547974\n",
      "330 1.03182852268219\n",
      "331 1.0317026376724243\n",
      "332 1.031577467918396\n",
      "333 1.0314528942108154\n",
      "334 1.031327486038208\n",
      "335 1.0312016010284424\n",
      "336 1.0310766696929932\n",
      "337 1.0309511423110962\n",
      "338 1.0308265686035156\n",
      "339 1.030701756477356\n",
      "340 1.0305763483047485\n",
      "341 1.0304511785507202\n",
      "342 1.03032648563385\n",
      "343 1.0302011966705322\n",
      "344 1.030076265335083\n",
      "345 1.029951810836792\n",
      "346 1.0298274755477905\n",
      "347 1.0297026634216309\n",
      "348 1.0295770168304443\n",
      "349 1.0294525623321533\n",
      "350 1.029327154159546\n",
      "351 1.0292025804519653\n",
      "352 1.0290783643722534\n",
      "353 1.0289541482925415\n",
      "354 1.0288290977478027\n",
      "355 1.0287045240402222\n",
      "356 1.0285797119140625\n",
      "357 1.0284550189971924\n",
      "358 1.0283310413360596\n",
      "359 1.0282062292099\n",
      "360 1.0280821323394775\n",
      "361 1.027957558631897\n",
      "362 1.0278329849243164\n",
      "363 1.0277091264724731\n",
      "364 1.0275843143463135\n",
      "365 1.0274596214294434\n",
      "366 1.0273361206054688\n",
      "367 1.0272127389907837\n",
      "368 1.027087926864624\n",
      "369 1.0269629955291748\n",
      "370 1.026838779449463\n",
      "371 1.026714563369751\n",
      "372 1.0265910625457764\n",
      "373 1.0264666080474854\n",
      "374 1.0263428688049316\n",
      "375 1.0262184143066406\n",
      "376 1.0260939598083496\n",
      "377 1.025970697402954\n",
      "378 1.0258466005325317\n",
      "379 1.0257223844528198\n",
      "380 1.0255992412567139\n",
      "381 1.0254746675491333\n",
      "382 1.0253510475158691\n",
      "383 1.025227427482605\n",
      "384 1.0251039266586304\n",
      "385 1.024980068206787\n",
      "386 1.0248557329177856\n",
      "387 1.0247315168380737\n",
      "388 1.02460777759552\n",
      "389 1.0244841575622559\n",
      "390 1.0243608951568604\n",
      "391 1.0242369174957275\n",
      "392 1.0241129398345947\n",
      "393 1.0239899158477783\n",
      "394 1.0238654613494873\n",
      "395 1.0237419605255127\n",
      "396 1.0236185789108276\n",
      "397 1.0234942436218262\n",
      "398 1.0233720541000366\n",
      "399 1.0232478380203247\n",
      "400 1.0231244564056396\n",
      "401 1.0230004787445068\n",
      "402 1.022876501083374\n",
      "403 1.0227534770965576\n",
      "404 1.0226300954818726\n",
      "405 1.0225062370300293\n",
      "406 1.022383213043213\n",
      "407 1.0222601890563965\n",
      "408 1.0221370458602905\n",
      "409 1.02201247215271\n",
      "410 1.02189040184021\n",
      "411 1.0217669010162354\n",
      "412 1.0216437578201294\n",
      "413 1.0215197801589966\n",
      "414 1.0213967561721802\n",
      "415 1.0212732553482056\n",
      "416 1.021150827407837\n",
      "417 1.021026849746704\n",
      "418 1.0209041833877563\n",
      "419 1.0207805633544922\n",
      "420 1.0206577777862549\n",
      "421 1.020534873008728\n",
      "422 1.020411729812622\n",
      "423 1.0202887058258057\n",
      "424 1.020166277885437\n",
      "425 1.0200427770614624\n",
      "426 1.0199201107025146\n",
      "427 1.0197969675064087\n",
      "428 1.01967453956604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429 1.019552230834961\n",
      "430 1.019429326057434\n",
      "431 1.0193067789077759\n",
      "432 1.0191847085952759\n",
      "433 1.0190613269805908\n",
      "434 1.0189383029937744\n",
      "435 1.0188158750534058\n",
      "436 1.018693208694458\n",
      "437 1.0185710191726685\n",
      "438 1.0184481143951416\n",
      "439 1.018325924873352\n",
      "440 1.0182030200958252\n",
      "441 1.0180809497833252\n",
      "442 1.0179591178894043\n",
      "443 1.0178356170654297\n",
      "444 1.017714262008667\n",
      "445 1.0175920724868774\n",
      "446 1.0174691677093506\n",
      "447 1.0173470973968506\n",
      "448 1.0172253847122192\n",
      "449 1.017102599143982\n",
      "450 1.0169798135757446\n",
      "451 1.0168578624725342\n",
      "452 1.0167365074157715\n",
      "453 1.0166137218475342\n",
      "454 1.0164918899536133\n",
      "455 1.0163695812225342\n",
      "456 1.0162477493286133\n",
      "457 1.0161244869232178\n",
      "458 1.0160032510757446\n",
      "459 1.0158816576004028\n",
      "460 1.0157593488693237\n",
      "461 1.0156375169754028\n",
      "462 1.0155150890350342\n",
      "463 1.015393614768982\n",
      "464 1.0152720212936401\n",
      "465 1.0151498317718506\n",
      "466 1.0150272846221924\n",
      "467 1.0149056911468506\n",
      "468 1.0147838592529297\n",
      "469 1.0146623849868774\n",
      "470 1.014540195465088\n",
      "471 1.0144197940826416\n",
      "472 1.0142974853515625\n",
      "473 1.0141751766204834\n",
      "474 1.0140535831451416\n",
      "475 1.013932466506958\n",
      "476 1.013810157775879\n",
      "477 1.0136892795562744\n",
      "478 1.0135676860809326\n",
      "479 1.0134456157684326\n",
      "480 1.0133240222930908\n",
      "481 1.013202428817749\n",
      "482 1.0130813121795654\n",
      "483 1.0129591226577759\n",
      "484 1.0128381252288818\n",
      "485 1.012717366218567\n",
      "486 1.0125954151153564\n",
      "487 1.0124733448028564\n",
      "488 1.0123517513275146\n",
      "489 1.0122302770614624\n",
      "490 1.0121092796325684\n",
      "491 1.0119874477386475\n",
      "492 1.011866807937622\n",
      "493 1.0117450952529907\n",
      "494 1.0116238594055176\n",
      "495 1.011502981185913\n",
      "496 1.01138174533844\n",
      "497 1.0112603902816772\n",
      "498 1.0111395120620728\n",
      "499 1.0110185146331787\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "\n",
    "# criterion is a loss function.\n",
    "# In essence, it is a Function Operation.\n",
    "criterion = torch.nn.MSELoss() # reduction='sum'\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "for t in range(500):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad() # prepare to calculate the grad\n",
    "    loss.backward()       # backward to get the grad\n",
    "    optimizer.step()      # add the grad to weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
