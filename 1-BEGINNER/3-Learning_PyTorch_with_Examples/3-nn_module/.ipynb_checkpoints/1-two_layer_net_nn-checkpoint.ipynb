{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: nn\n",
    "-----------\n",
    "\n",
    "**AIM:**\n",
    "\n",
    "A fully-connected ReLU network with one hidden layer, trained to predict y from x\n",
    "by minimizing squared Euclidean distance.\n",
    "\n",
    "**Method:**\n",
    "\n",
    "This implementation uses the `nn` package from PyTorch to build the network.\n",
    "\n",
    "PyTorch `autograd` makes it easy to define computational graphs and take gradients,\n",
    "\n",
    "but raw `autograd` can be a bit too **low-level** for defining complex neural networks.\n",
    "\n",
    "\n",
    "This is where the `nn` package can help. \n",
    "\n",
    "The `nn` package defines a set of Modules, which you can think of as a neural network layer that has produces output from **input** and may have some **trainable weights**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use the nn package to define our model as a sequence of layers. \n",
    "\n",
    "# nn.Sequential is a Module which contains other Modules, \n",
    "# and applies them in sequence to produce its output. \n",
    "# Each Linear Module computes output from input using a linear function, \n",
    "# and holds internal Tensors for its weight and bias.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H), # w1:(D_in, H)\n",
    "    torch.nn.ReLU(),          # ReLU\n",
    "    torch.nn.Linear(H, D_out),# w2:(H, D_out) \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# The `nn` package also contains definitions of popular loss functions; \n",
    "# in this case \n",
    "# we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss() # not reduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0910389423370361\n",
      "1 1.0909075736999512\n",
      "2 1.090775489807129\n",
      "3 1.090644121170044\n",
      "4 1.0905132293701172\n",
      "5 1.0903816223144531\n",
      "6 1.0902498960494995\n",
      "7 1.090118646621704\n",
      "8 1.0899873971939087\n",
      "9 1.0898566246032715\n",
      "10 1.0897254943847656\n",
      "11 1.0895953178405762\n",
      "12 1.0894638299942017\n",
      "13 1.089333176612854\n",
      "14 1.0892020463943481\n",
      "15 1.0890710353851318\n",
      "16 1.0889408588409424\n",
      "17 1.0888090133666992\n",
      "18 1.0886787176132202\n",
      "19 1.088547945022583\n",
      "20 1.0884170532226562\n",
      "21 1.0882854461669922\n",
      "22 1.0881550312042236\n",
      "23 1.0880248546600342\n",
      "24 1.0878931283950806\n",
      "25 1.0877630710601807\n",
      "26 1.0876328945159912\n",
      "27 1.0875022411346436\n",
      "28 1.0873714685440063\n",
      "29 1.0872410535812378\n",
      "30 1.0871111154556274\n",
      "31 1.086979627609253\n",
      "32 1.0868501663208008\n",
      "33 1.0867195129394531\n",
      "34 1.086588978767395\n",
      "35 1.086458683013916\n",
      "36 1.0863277912139893\n",
      "37 1.0861976146697998\n",
      "38 1.0860679149627686\n",
      "39 1.0859379768371582\n",
      "40 1.0858073234558105\n",
      "41 1.0856773853302002\n",
      "42 1.0855475664138794\n",
      "43 1.0854172706604004\n",
      "44 1.0852876901626587\n",
      "45 1.0851573944091797\n",
      "46 1.0850274562835693\n",
      "47 1.084897756576538\n",
      "48 1.0847671031951904\n",
      "49 1.0846376419067383\n",
      "50 1.084507703781128\n",
      "51 1.0843768119812012\n",
      "52 1.0842469930648804\n",
      "53 1.08411705493927\n",
      "54 1.0839874744415283\n",
      "55 1.083858609199524\n",
      "56 1.0837290287017822\n",
      "57 1.0836000442504883\n",
      "58 1.0834712982177734\n",
      "59 1.0833423137664795\n",
      "60 1.0832128524780273\n",
      "61 1.0830835103988647\n",
      "62 1.0829548835754395\n",
      "63 1.0828266143798828\n",
      "64 1.0826972723007202\n",
      "65 1.0825682878494263\n",
      "66 1.08243989944458\n",
      "67 1.0823109149932861\n",
      "68 1.0821818113327026\n",
      "69 1.082053303718567\n",
      "70 1.0819238424301147\n",
      "71 1.0817954540252686\n",
      "72 1.081667184829712\n",
      "73 1.0815383195877075\n",
      "74 1.0814095735549927\n",
      "75 1.081281065940857\n",
      "76 1.0811522006988525\n",
      "77 1.0810240507125854\n",
      "78 1.080895185470581\n",
      "79 1.0807673931121826\n",
      "80 1.0806394815444946\n",
      "81 1.080509901046753\n",
      "82 1.080382227897644\n",
      "83 1.0802536010742188\n",
      "84 1.0801256895065308\n",
      "85 1.0799968242645264\n",
      "86 1.079867959022522\n",
      "87 1.0797407627105713\n",
      "88 1.079611897468567\n",
      "89 1.0794837474822998\n",
      "90 1.0793551206588745\n",
      "91 1.079227328300476\n",
      "92 1.0790995359420776\n",
      "93 1.078971266746521\n",
      "94 1.0788432359695435\n",
      "95 1.0787146091461182\n",
      "96 1.0785876512527466\n",
      "97 1.0784592628479004\n",
      "98 1.0783311128616333\n",
      "99 1.0782041549682617\n",
      "100 1.078075647354126\n",
      "101 1.0779476165771484\n",
      "102 1.077820062637329\n",
      "103 1.0776917934417725\n",
      "104 1.0775644779205322\n",
      "105 1.0774378776550293\n",
      "106 1.077309250831604\n",
      "107 1.0771820545196533\n",
      "108 1.0770543813705444\n",
      "109 1.0769269466400146\n",
      "110 1.0767993927001953\n",
      "111 1.0766725540161133\n",
      "112 1.0765445232391357\n",
      "113 1.0764179229736328\n",
      "114 1.0762908458709717\n",
      "115 1.0761626958847046\n",
      "116 1.0760349035263062\n",
      "117 1.075908899307251\n",
      "118 1.0757813453674316\n",
      "119 1.0756546258926392\n",
      "120 1.075527310371399\n",
      "121 1.07539963722229\n",
      "122 1.0752732753753662\n",
      "123 1.075146198272705\n",
      "124 1.0750195980072021\n",
      "125 1.074892282485962\n",
      "126 1.0747654438018799\n",
      "127 1.0746376514434814\n",
      "128 1.0745112895965576\n",
      "129 1.0743849277496338\n",
      "130 1.074257493019104\n",
      "131 1.0741307735443115\n",
      "132 1.0740034580230713\n",
      "133 1.0738767385482788\n",
      "134 1.0737502574920654\n",
      "135 1.0736232995986938\n",
      "136 1.0734971761703491\n",
      "137 1.0733706951141357\n",
      "138 1.0732446908950806\n",
      "139 1.0731174945831299\n",
      "140 1.072991132736206\n",
      "141 1.0728644132614136\n",
      "142 1.072737455368042\n",
      "143 1.0726125240325928\n",
      "144 1.0724856853485107\n",
      "145 1.0723598003387451\n",
      "146 1.072234034538269\n",
      "147 1.0721065998077393\n",
      "148 1.0719823837280273\n",
      "149 1.0718557834625244\n",
      "150 1.0717294216156006\n",
      "151 1.071603775024414\n",
      "152 1.0714776515960693\n",
      "153 1.0713521242141724\n",
      "154 1.0712261199951172\n",
      "155 1.0711004734039307\n",
      "156 1.0709750652313232\n",
      "157 1.0708481073379517\n",
      "158 1.070723056793213\n",
      "159 1.070596694946289\n",
      "160 1.0704708099365234\n",
      "161 1.0703457593917847\n",
      "162 1.0702195167541504\n",
      "163 1.070094108581543\n",
      "164 1.0699687004089355\n",
      "165 1.0698426961898804\n",
      "166 1.0697177648544312\n",
      "167 1.069591760635376\n",
      "168 1.0694659948349\n",
      "169 1.0693399906158447\n",
      "170 1.0692143440246582\n",
      "171 1.0690891742706299\n",
      "172 1.0689644813537598\n",
      "173 1.068838357925415\n",
      "174 1.0687129497528076\n",
      "175 1.0685865879058838\n",
      "176 1.0684608221054077\n",
      "177 1.068337082862854\n",
      "178 1.0682114362716675\n",
      "179 1.0680863857269287\n",
      "180 1.0679608583450317\n",
      "181 1.067834734916687\n",
      "182 1.06770920753479\n",
      "183 1.0675854682922363\n",
      "184 1.0674593448638916\n",
      "185 1.0673339366912842\n",
      "186 1.0672085285186768\n",
      "187 1.067083716392517\n",
      "188 1.0669578313827515\n",
      "189 1.06683349609375\n",
      "190 1.0667083263397217\n",
      "191 1.0665831565856934\n",
      "192 1.0664576292037964\n",
      "193 1.066333293914795\n",
      "194 1.0662078857421875\n",
      "195 1.0660825967788696\n",
      "196 1.0659583806991577\n",
      "197 1.0658336877822876\n",
      "198 1.0657093524932861\n",
      "199 1.0655845403671265\n",
      "200 1.065460443496704\n",
      "201 1.0653352737426758\n",
      "202 1.0652105808258057\n",
      "203 1.0650856494903564\n",
      "204 1.0649611949920654\n",
      "205 1.0648367404937744\n",
      "206 1.0647122859954834\n",
      "207 1.0645880699157715\n",
      "208 1.0644630193710327\n",
      "209 1.0643389225006104\n",
      "210 1.0642143487930298\n",
      "211 1.064089298248291\n",
      "212 1.063966155052185\n",
      "213 1.0638408660888672\n",
      "214 1.0637164115905762\n",
      "215 1.0635919570922852\n",
      "216 1.0634678602218628\n",
      "217 1.0633437633514404\n",
      "218 1.063219666481018\n",
      "219 1.063095211982727\n",
      "220 1.0629711151123047\n",
      "221 1.0628464221954346\n",
      "222 1.0627223253250122\n",
      "223 1.0625985860824585\n",
      "224 1.062474012374878\n",
      "225 1.0623507499694824\n",
      "226 1.0622256994247437\n",
      "227 1.0621024370193481\n",
      "228 1.0619778633117676\n",
      "229 1.0618544816970825\n",
      "230 1.0617296695709229\n",
      "231 1.0616061687469482\n",
      "232 1.0614815950393677\n",
      "233 1.061357855796814\n",
      "234 1.0612342357635498\n",
      "235 1.0611101388931274\n",
      "236 1.0609861612319946\n",
      "237 1.0608638525009155\n",
      "238 1.0607393980026245\n",
      "239 1.0606160163879395\n",
      "240 1.0604915618896484\n",
      "241 1.0603684186935425\n",
      "242 1.0602447986602783\n",
      "243 1.060121774673462\n",
      "244 1.0599980354309082\n",
      "245 1.059874176979065\n",
      "246 1.0597501993179321\n",
      "247 1.0596270561218262\n",
      "248 1.0595037937164307\n",
      "249 1.059380292892456\n",
      "250 1.0592567920684814\n",
      "251 1.0591331720352173\n",
      "252 1.05901038646698\n",
      "253 1.058885931968689\n",
      "254 1.0587631464004517\n",
      "255 1.0586390495300293\n",
      "256 1.0585169792175293\n",
      "257 1.0583934783935547\n",
      "258 1.0582702159881592\n",
      "259 1.0581467151641846\n",
      "260 1.0580236911773682\n",
      "261 1.05790114402771\n",
      "262 1.0577776432037354\n",
      "263 1.0576541423797607\n",
      "264 1.0575308799743652\n",
      "265 1.0574074983596802\n",
      "266 1.0572845935821533\n",
      "267 1.0571616888046265\n",
      "268 1.0570383071899414\n",
      "269 1.0569157600402832\n",
      "270 1.0567924976348877\n",
      "271 1.0566705465316772\n",
      "272 1.056546926498413\n",
      "273 1.0564240217208862\n",
      "274 1.0563007593154907\n",
      "275 1.0561784505844116\n",
      "276 1.0560557842254639\n",
      "277 1.0559322834014893\n",
      "278 1.055809736251831\n",
      "279 1.0556868314743042\n",
      "280 1.0555639266967773\n",
      "281 1.05544114112854\n",
      "282 1.0553178787231445\n",
      "283 1.0551955699920654\n",
      "284 1.0550731420516968\n",
      "285 1.0549499988555908\n",
      "286 1.0548276901245117\n",
      "287 1.0547046661376953\n",
      "288 1.0545819997787476\n",
      "289 1.0544601678848267\n",
      "290 1.0543367862701416\n",
      "291 1.0542142391204834\n",
      "292 1.054092288017273\n",
      "293 1.053969144821167\n",
      "294 1.0538475513458252\n",
      "295 1.0537248849868774\n",
      "296 1.0536019802093506\n",
      "297 1.0534796714782715\n",
      "298 1.0533583164215088\n",
      "299 1.053234577178955\n",
      "300 1.053112268447876\n",
      "301 1.0529900789260864\n",
      "302 1.0528680086135864\n",
      "303 1.0527453422546387\n",
      "304 1.0526232719421387\n",
      "305 1.0525002479553223\n",
      "306 1.0523781776428223\n",
      "307 1.0522558689117432\n",
      "308 1.0521336793899536\n",
      "309 1.0520110130310059\n",
      "310 1.051889419555664\n",
      "311 1.0517667531967163\n",
      "312 1.0516445636749268\n",
      "313 1.0515230894088745\n",
      "314 1.0514004230499268\n",
      "315 1.0512789487838745\n",
      "316 1.051156759262085\n",
      "317 1.051034927368164\n",
      "318 1.0509123802185059\n",
      "319 1.050790786743164\n",
      "320 1.0506685972213745\n",
      "321 1.0505462884902954\n",
      "322 1.0504246950149536\n",
      "323 1.050303339958191\n",
      "324 1.05018150806427\n",
      "325 1.0500601530075073\n",
      "326 1.0499382019042969\n",
      "327 1.0498167276382446\n",
      "328 1.0496947765350342\n",
      "329 1.0495727062225342\n",
      "330 1.0494520664215088\n",
      "331 1.049329161643982\n",
      "332 1.0492091178894043\n",
      "333 1.0490878820419312\n",
      "334 1.0489665269851685\n",
      "335 1.0488454103469849\n",
      "336 1.048723816871643\n",
      "337 1.0486024618148804\n",
      "338 1.0484809875488281\n",
      "339 1.048359990119934\n",
      "340 1.0482375621795654\n",
      "341 1.048116683959961\n",
      "342 1.047995924949646\n",
      "343 1.0478737354278564\n",
      "344 1.047752857208252\n",
      "345 1.0476315021514893\n",
      "346 1.0475102663040161\n",
      "347 1.0473898649215698\n",
      "348 1.0472676753997803\n",
      "349 1.0471470355987549\n",
      "350 1.0470253229141235\n",
      "351 1.0469049215316772\n",
      "352 1.0467830896377563\n",
      "353 1.0466628074645996\n",
      "354 1.0465409755706787\n",
      "355 1.0464208126068115\n",
      "356 1.0462992191314697\n",
      "357 1.0461782217025757\n",
      "358 1.046058177947998\n",
      "359 1.0459363460540771\n",
      "360 1.0458152294158936\n",
      "361 1.0456945896148682\n",
      "362 1.0455745458602905\n",
      "363 1.0454530715942383\n",
      "364 1.045332431793213\n",
      "365 1.045211672782898\n",
      "366 1.045090913772583\n",
      "367 1.0449703931808472\n",
      "368 1.0448503494262695\n",
      "369 1.0447280406951904\n",
      "370 1.044608235359192\n",
      "371 1.0444875955581665\n",
      "372 1.0443665981292725\n",
      "373 1.0442460775375366\n",
      "374 1.0441253185272217\n",
      "375 1.0440049171447754\n",
      "376 1.0438849925994873\n",
      "377 1.0437644720077515\n",
      "378 1.0436433553695679\n",
      "379 1.0435231924057007\n",
      "380 1.0434021949768066\n",
      "381 1.043282389640808\n",
      "382 1.043161153793335\n",
      "383 1.0430413484573364\n",
      "384 1.0429203510284424\n",
      "385 1.042800784111023\n",
      "386 1.0426799058914185\n",
      "387 1.042559266090393\n",
      "388 1.0424394607543945\n",
      "389 1.0423191785812378\n",
      "390 1.0421993732452393\n",
      "391 1.0420786142349243\n",
      "392 1.0419590473175049\n",
      "393 1.0418384075164795\n",
      "394 1.041717529296875\n",
      "395 1.0415980815887451\n",
      "396 1.0414783954620361\n",
      "397 1.0413578748703003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398 1.0412371158599854\n",
      "399 1.0411176681518555\n",
      "400 1.04099702835083\n",
      "401 1.0408774614334106\n",
      "402 1.0407578945159912\n",
      "403 1.0406372547149658\n",
      "404 1.0405170917510986\n",
      "405 1.0403974056243896\n",
      "406 1.040277361869812\n",
      "407 1.0401575565338135\n",
      "408 1.040037989616394\n",
      "409 1.0399178266525269\n",
      "410 1.0397976636886597\n",
      "411 1.0396780967712402\n",
      "412 1.0395584106445312\n",
      "413 1.0394388437271118\n",
      "414 1.0393178462982178\n",
      "415 1.039198637008667\n",
      "416 1.0390795469284058\n",
      "417 1.0389583110809326\n",
      "418 1.0388399362564087\n",
      "419 1.038719654083252\n",
      "420 1.0386000871658325\n",
      "421 1.038480281829834\n",
      "422 1.0383611917495728\n",
      "423 1.0382411479949951\n",
      "424 1.0381221771240234\n",
      "425 1.038001537322998\n",
      "426 1.0378823280334473\n",
      "427 1.0377628803253174\n",
      "428 1.0376436710357666\n",
      "429 1.0375245809555054\n",
      "430 1.0374048948287964\n",
      "431 1.0372856855392456\n",
      "432 1.0371663570404053\n",
      "433 1.0370476245880127\n",
      "434 1.0369280576705933\n",
      "435 1.0368082523345947\n",
      "436 1.03669011592865\n",
      "437 1.0365700721740723\n",
      "438 1.0364515781402588\n",
      "439 1.0363315343856812\n",
      "440 1.0362133979797363\n",
      "441 1.0360944271087646\n",
      "442 1.0359747409820557\n",
      "443 1.0358556509017944\n",
      "444 1.0357367992401123\n",
      "445 1.0356178283691406\n",
      "446 1.035499095916748\n",
      "447 1.035380482673645\n",
      "448 1.0352613925933838\n",
      "449 1.0351417064666748\n",
      "450 1.035023808479309\n",
      "451 1.0349044799804688\n",
      "452 1.0347867012023926\n",
      "453 1.034667730331421\n",
      "454 1.034549593925476\n",
      "455 1.034430980682373\n",
      "456 1.0343118906021118\n",
      "457 1.0341936349868774\n",
      "458 1.034075140953064\n",
      "459 1.0339574813842773\n",
      "460 1.0338386297225952\n",
      "461 1.0337202548980713\n",
      "462 1.0336023569107056\n",
      "463 1.0334837436676025\n",
      "464 1.0333647727966309\n",
      "465 1.0332469940185547\n",
      "466 1.0331284999847412\n",
      "467 1.0330097675323486\n",
      "468 1.0328923463821411\n",
      "469 1.0327739715576172\n",
      "470 1.0326553583145142\n",
      "471 1.032537579536438\n",
      "472 1.0324195623397827\n",
      "473 1.032301664352417\n",
      "474 1.0321829319000244\n",
      "475 1.0320653915405273\n",
      "476 1.0319478511810303\n",
      "477 1.0318310260772705\n",
      "478 1.0317124128341675\n",
      "479 1.0315942764282227\n",
      "480 1.0314768552780151\n",
      "481 1.0313595533370972\n",
      "482 1.031241536140442\n",
      "483 1.031124234199524\n",
      "484 1.031006932258606\n",
      "485 1.0308887958526611\n",
      "486 1.0307719707489014\n",
      "487 1.030653953552246\n",
      "488 1.0305368900299072\n",
      "489 1.0304195880889893\n",
      "490 1.0303019285202026\n",
      "491 1.030184030532837\n",
      "492 1.030066728591919\n",
      "493 1.02994966506958\n",
      "494 1.0298322439193726\n",
      "495 1.029714822769165\n",
      "496 1.029597282409668\n",
      "497 1.0294800996780396\n",
      "498 1.0293635129928589\n",
      "499 1.0292460918426514\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learning_rate = 1e-4\n",
    "\n",
    "for t in range(500):\n",
    "    # Forward pass: \n",
    "    # compute predicted y by passing x to the model. \n",
    "    # Module objects override the __call__ operator \n",
    "    # so you can call them like functions. \n",
    "    # When doing so you pass a Tensor of input data to the Module \n",
    "    # and it produces a Tensor of output data.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    # gradients is d(loss)/d(w1), or d(loss)/d(w2)\n",
    "    # set it to zero before the backward pass\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: \n",
    "    # compute gradient of the loss with respect to \n",
    "    # all the learnable parameters of the model. \n",
    "    # Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, \n",
    "    # so this call will compute gradients for all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters(): # param: w1 and w2\n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
