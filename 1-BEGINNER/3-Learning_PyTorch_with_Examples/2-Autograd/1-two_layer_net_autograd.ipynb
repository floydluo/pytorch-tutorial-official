{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: Tensors and autograd\n",
    "-------------------------------\n",
    "\n",
    "A fully-connected ReLU network with one hidden layer and no biases, trained to\n",
    "predict y from x by minimizing squared Euclidean distance.\n",
    "\n",
    "This implementation computes the forward pass using operations on PyTorch\n",
    "Tensors, and uses **PyTorch autograd** to compute gradients.\n",
    "\n",
    "\n",
    "A PyTorch Tensor represents a node in a computational graph. \n",
    "\n",
    "If ``x`` is a Tensor that has ``x.requires_grad=True`` \n",
    "\n",
    "then ``x.grad`` is another Tensor holding the gradient of ``x`` with respect to some scalar value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1000])\n",
      "torch.Size([64, 10])\n",
      "torch.Size([1000, 100])\n",
      "torch.Size([100, 10])\n"
     ]
    }
   ],
   "source": [
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "# with respect to these Tensors during the backward pass.\n",
    "x = torch.randn(N, D_in,  device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "\n",
    "### NOTICE HERE\n",
    "# w1, w2 are the parameter to learn\n",
    "# so they are require the grad to be true\n",
    "w1 = torch.randn(D_in, H, device=device,  dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(w1.shape)\n",
    "print(w2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 46927512.0\n",
      "1 60032444.0\n",
      "2 76812616.0\n",
      "3 69671048.0\n",
      "4 35941444.0\n",
      "5 10474588.0\n",
      "6 3325159.25\n",
      "7 1904509.125\n",
      "8 1461474.0\n",
      "9 1201787.0\n",
      "10 1007274.625\n",
      "11 852703.5\n",
      "12 727583.125\n",
      "13 625124.375\n",
      "14 540337.4375\n",
      "15 469663.5\n",
      "16 410427.21875\n",
      "17 360341.5\n",
      "18 317774.9375\n",
      "19 281407.65625\n",
      "20 250136.15625\n",
      "21 223125.359375\n",
      "22 199669.0\n",
      "23 179207.0625\n",
      "24 161281.421875\n",
      "25 145516.25\n",
      "26 131615.984375\n",
      "27 119318.484375\n",
      "28 108397.453125\n",
      "29 98673.59375\n",
      "30 89997.3046875\n",
      "31 82232.9609375\n",
      "32 75268.4609375\n",
      "33 69004.734375\n",
      "34 63357.2890625\n",
      "35 58255.51171875\n",
      "36 53642.1171875\n",
      "37 49458.63671875\n",
      "38 45659.48828125\n",
      "39 42206.44140625\n",
      "40 39060.51953125\n",
      "41 36192.4296875\n",
      "42 33572.06640625\n",
      "43 31173.400390625\n",
      "44 28975.78515625\n",
      "45 26958.248046875\n",
      "46 25103.654296875\n",
      "47 23397.26171875\n",
      "48 21824.580078125\n",
      "49 20373.853515625\n",
      "50 19033.31640625\n",
      "51 17793.849609375\n",
      "52 16647.544921875\n",
      "53 15585.228515625\n",
      "54 14599.814453125\n",
      "55 13685.3408203125\n",
      "56 12837.3388671875\n",
      "57 12048.8193359375\n",
      "58 11314.486328125\n",
      "59 10631.3046875\n",
      "60 9995.5732421875\n",
      "61 9401.4482421875\n",
      "62 8847.08203125\n",
      "63 8329.298828125\n",
      "64 7845.255859375\n",
      "65 7392.75\n",
      "66 6969.29345703125\n",
      "67 6572.79736328125\n",
      "68 6201.68994140625\n",
      "69 5853.798828125\n",
      "70 5527.5234375\n",
      "71 5221.36474609375\n",
      "72 4933.95849609375\n",
      "73 4663.982421875\n",
      "74 4410.1572265625\n",
      "75 4171.53515625\n",
      "76 3947.14697265625\n",
      "77 3735.897216796875\n",
      "78 3537.10595703125\n",
      "79 3350.05908203125\n",
      "80 3173.937255859375\n",
      "81 3007.952880859375\n",
      "82 2851.390625\n",
      "83 2703.71484375\n",
      "84 2564.42431640625\n",
      "85 2432.919677734375\n",
      "86 2308.75048828125\n",
      "87 2191.40087890625\n",
      "88 2080.490478515625\n",
      "89 1975.6903076171875\n",
      "90 1876.6370849609375\n",
      "91 1782.9151611328125\n",
      "92 1694.182373046875\n",
      "93 1610.1932373046875\n",
      "94 1530.7230224609375\n",
      "95 1455.427734375\n",
      "96 1384.117431640625\n",
      "97 1316.58544921875\n",
      "98 1252.5601806640625\n",
      "99 1191.8739013671875\n",
      "100 1134.3212890625\n",
      "101 1079.741455078125\n",
      "102 1027.9788818359375\n",
      "103 978.8703002929688\n",
      "104 932.2723999023438\n",
      "105 888.0203857421875\n",
      "106 846.006591796875\n",
      "107 806.1141967773438\n",
      "108 768.2088623046875\n",
      "109 732.2163696289062\n",
      "110 698.0155029296875\n",
      "111 665.5185546875\n",
      "112 634.6102294921875\n",
      "113 605.2380981445312\n",
      "114 577.3021240234375\n",
      "115 550.7256469726562\n",
      "116 525.4447021484375\n",
      "117 501.396728515625\n",
      "118 478.5138854980469\n",
      "119 456.7351989746094\n",
      "120 435.99932861328125\n",
      "121 416.2659606933594\n",
      "122 397.4651794433594\n",
      "123 379.5643310546875\n",
      "124 362.5180969238281\n",
      "125 346.27862548828125\n",
      "126 330.8026428222656\n",
      "127 316.05517578125\n",
      "128 301.999267578125\n",
      "129 288.5985412597656\n",
      "130 275.8244934082031\n",
      "131 263.6456604003906\n",
      "132 252.0306396484375\n",
      "133 240.95677185058594\n",
      "134 230.3910675048828\n",
      "135 220.31040954589844\n",
      "136 210.69729614257812\n",
      "137 201.52047729492188\n",
      "138 192.7637939453125\n",
      "139 184.4051513671875\n",
      "140 176.42404174804688\n",
      "141 168.80410766601562\n",
      "142 161.5308837890625\n",
      "143 154.5835418701172\n",
      "144 147.9479522705078\n",
      "145 141.61199951171875\n",
      "146 135.56092834472656\n",
      "147 129.7771759033203\n",
      "148 124.25030517578125\n",
      "149 118.97227478027344\n",
      "150 113.92485809326172\n",
      "151 109.10265350341797\n",
      "152 104.49166870117188\n",
      "153 100.08551025390625\n",
      "154 95.87135314941406\n",
      "155 91.84391784667969\n",
      "156 87.99132537841797\n",
      "157 84.30682373046875\n",
      "158 80.78189086914062\n",
      "159 77.41104125976562\n",
      "160 74.1874008178711\n",
      "161 71.10348510742188\n",
      "162 68.15158081054688\n",
      "163 65.32693481445312\n",
      "164 62.62352752685547\n",
      "165 60.03519058227539\n",
      "166 57.5596923828125\n",
      "167 55.18949890136719\n",
      "168 52.92138671875\n",
      "169 50.74867630004883\n",
      "170 48.66937255859375\n",
      "171 46.6776008605957\n",
      "172 44.77047348022461\n",
      "173 42.943809509277344\n",
      "174 41.19515609741211\n",
      "175 39.51957702636719\n",
      "176 37.91519546508789\n",
      "177 36.37715148925781\n",
      "178 34.90412139892578\n",
      "179 33.49211120605469\n",
      "180 32.139808654785156\n",
      "181 30.8439884185791\n",
      "182 29.600847244262695\n",
      "183 28.41083335876465\n",
      "184 27.27014923095703\n",
      "185 26.175966262817383\n",
      "186 25.127769470214844\n",
      "187 24.122543334960938\n",
      "188 23.158658981323242\n",
      "189 22.23474884033203\n",
      "190 21.348220825195312\n",
      "191 20.499420166015625\n",
      "192 19.684659957885742\n",
      "193 18.902873992919922\n",
      "194 18.153553009033203\n",
      "195 17.434606552124023\n",
      "196 16.744667053222656\n",
      "197 16.083160400390625\n",
      "198 15.448956489562988\n",
      "199 14.839835166931152\n",
      "200 14.255555152893066\n",
      "201 13.695042610168457\n",
      "202 13.157489776611328\n",
      "203 12.641426086425781\n",
      "204 12.145742416381836\n",
      "205 11.670869827270508\n",
      "206 11.21433162689209\n",
      "207 10.775893211364746\n",
      "208 10.355727195739746\n",
      "209 9.952062606811523\n",
      "210 9.564436912536621\n",
      "211 9.192498207092285\n",
      "212 8.835382461547852\n",
      "213 8.49266529083252\n",
      "214 8.163272857666016\n",
      "215 7.84718656539917\n",
      "216 7.543712139129639\n",
      "217 7.251943588256836\n",
      "218 6.971977710723877\n",
      "219 6.702917575836182\n",
      "220 6.444741725921631\n",
      "221 6.196545124053955\n",
      "222 5.95813512802124\n",
      "223 5.7292399406433105\n",
      "224 5.509385585784912\n",
      "225 5.29805850982666\n",
      "226 5.095006942749023\n",
      "227 4.899904251098633\n",
      "228 4.712677001953125\n",
      "229 4.53242826461792\n",
      "230 4.359446048736572\n",
      "231 4.193153381347656\n",
      "232 4.033392429351807\n",
      "233 3.8798704147338867\n",
      "234 3.7320127487182617\n",
      "235 3.5902366638183594\n",
      "236 3.4539151191711426\n",
      "237 3.3229358196258545\n",
      "238 3.1968092918395996\n",
      "239 3.0756704807281494\n",
      "240 2.9592947959899902\n",
      "241 2.8473031520843506\n",
      "242 2.7397732734680176\n",
      "243 2.6363046169281006\n",
      "244 2.536886692047119\n",
      "245 2.44128155708313\n",
      "246 2.3492188453674316\n",
      "247 2.2607715129852295\n",
      "248 2.1757936477661133\n",
      "249 2.0939812660217285\n",
      "250 2.0152816772460938\n",
      "251 1.9396836757659912\n",
      "252 1.8668049573898315\n",
      "253 1.7969428300857544\n",
      "254 1.7296141386032104\n",
      "255 1.6648985147476196\n",
      "256 1.602699875831604\n",
      "257 1.5429238080978394\n",
      "258 1.485206127166748\n",
      "259 1.4298391342163086\n",
      "260 1.3764750957489014\n",
      "261 1.3251479864120483\n",
      "262 1.2759323120117188\n",
      "263 1.2284096479415894\n",
      "264 1.1827067136764526\n",
      "265 1.13872492313385\n",
      "266 1.0964668989181519\n",
      "267 1.0557690858840942\n",
      "268 1.0166707038879395\n",
      "269 0.9790204167366028\n",
      "270 0.942687451839447\n",
      "271 0.9078294634819031\n",
      "272 0.8742595911026001\n",
      "273 0.8419697880744934\n",
      "274 0.8108677268028259\n",
      "275 0.7809152603149414\n",
      "276 0.752067506313324\n",
      "277 0.7243595123291016\n",
      "278 0.6976421475410461\n",
      "279 0.6720261573791504\n",
      "280 0.6472888588905334\n",
      "281 0.6234787106513977\n",
      "282 0.6005617380142212\n",
      "283 0.5784488916397095\n",
      "284 0.557196855545044\n",
      "285 0.5367622375488281\n",
      "286 0.5171509981155396\n",
      "287 0.4982316493988037\n",
      "288 0.4798755645751953\n",
      "289 0.46236875653266907\n",
      "290 0.4454072117805481\n",
      "291 0.4291340112686157\n",
      "292 0.413443922996521\n",
      "293 0.39834725856781006\n",
      "294 0.3838135302066803\n",
      "295 0.36979609727859497\n",
      "296 0.35628947615623474\n",
      "297 0.3432939648628235\n",
      "298 0.3308013081550598\n",
      "299 0.3187575042247772\n",
      "300 0.3071584105491638\n",
      "301 0.2959984838962555\n",
      "302 0.28524014353752136\n",
      "303 0.2748924195766449\n",
      "304 0.26484522223472595\n",
      "305 0.25526249408721924\n",
      "306 0.24598179757595062\n",
      "307 0.2370537966489792\n",
      "308 0.22851213812828064\n",
      "309 0.22021999955177307\n",
      "310 0.21226747334003448\n",
      "311 0.20455171167850494\n",
      "312 0.1971706598997116\n",
      "313 0.19003364443778992\n",
      "314 0.18316435813903809\n",
      "315 0.17653116583824158\n",
      "316 0.17017698287963867\n",
      "317 0.16404429078102112\n",
      "318 0.15812529623508453\n",
      "319 0.15242218971252441\n",
      "320 0.14693821966648102\n",
      "321 0.14165739715099335\n",
      "322 0.13655273616313934\n",
      "323 0.13162009418010712\n",
      "324 0.12688899040222168\n",
      "325 0.12233489006757736\n",
      "326 0.117947056889534\n",
      "327 0.11370422691106796\n",
      "328 0.1096217930316925\n",
      "329 0.10567817091941833\n",
      "330 0.10190237313508987\n",
      "331 0.09824413061141968\n",
      "332 0.09470873326063156\n",
      "333 0.09133118391036987\n",
      "334 0.08805866539478302\n",
      "335 0.08490307629108429\n",
      "336 0.08186314254999161\n",
      "337 0.07895513623952866\n",
      "338 0.076144278049469\n",
      "339 0.07341654598712921\n",
      "340 0.07078629732131958\n",
      "341 0.06827396899461746\n",
      "342 0.06584695726633072\n",
      "343 0.06348451226949692\n",
      "344 0.061240874230861664\n",
      "345 0.05905494466423988\n",
      "346 0.05695801228284836\n",
      "347 0.05492500960826874\n",
      "348 0.052984897047281265\n",
      "349 0.05110235884785652\n",
      "350 0.04929498955607414\n",
      "351 0.047544658184051514\n",
      "352 0.04585889354348183\n",
      "353 0.044245604425668716\n",
      "354 0.04267381131649017\n",
      "355 0.04115549847483635\n",
      "356 0.039707448333501816\n",
      "357 0.03829628601670265\n",
      "358 0.03695114329457283\n",
      "359 0.03563931584358215\n",
      "360 0.03437273949384689\n",
      "361 0.03315947204828262\n",
      "362 0.03199988603591919\n",
      "363 0.03087502345442772\n",
      "364 0.02978399395942688\n",
      "365 0.02874283865094185\n",
      "366 0.027735333889722824\n",
      "367 0.026753149926662445\n",
      "368 0.025824006646871567\n",
      "369 0.02492019720375538\n",
      "370 0.02404598891735077\n",
      "371 0.023191144689917564\n",
      "372 0.02238643914461136\n",
      "373 0.021608851850032806\n",
      "374 0.020859137177467346\n",
      "375 0.02013418637216091\n",
      "376 0.019426584243774414\n",
      "377 0.018747756257653236\n",
      "378 0.0181000716984272\n",
      "379 0.017482077702879906\n",
      "380 0.016877584159374237\n",
      "381 0.016292762011289597\n",
      "382 0.015731438994407654\n",
      "383 0.015192507766187191\n",
      "384 0.014671605080366135\n",
      "385 0.014156446792185307\n",
      "386 0.013672217726707458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387 0.013214805163443089\n",
      "388 0.012761476449668407\n",
      "389 0.01232107076793909\n",
      "390 0.011899544857442379\n",
      "391 0.01149759441614151\n",
      "392 0.011105232872068882\n",
      "393 0.010726908221840858\n",
      "394 0.010365379974246025\n",
      "395 0.010015957988798618\n",
      "396 0.009675554931163788\n",
      "397 0.0093547273427248\n",
      "398 0.009034995920956135\n",
      "399 0.00873071514070034\n",
      "400 0.008439025841653347\n",
      "401 0.0081570940092206\n",
      "402 0.00788429006934166\n",
      "403 0.007625418249517679\n",
      "404 0.00737238023430109\n",
      "405 0.007127926219254732\n",
      "406 0.00689314492046833\n",
      "407 0.006669469177722931\n",
      "408 0.006445478182286024\n",
      "409 0.006234436761587858\n",
      "410 0.006030440330505371\n",
      "411 0.005836357828229666\n",
      "412 0.005644408520311117\n",
      "413 0.0054638502188026905\n",
      "414 0.0052838376723229885\n",
      "415 0.005116951651871204\n",
      "416 0.004951616283506155\n",
      "417 0.004790734965354204\n",
      "418 0.004640566185116768\n",
      "419 0.004493196494877338\n",
      "420 0.00435195118188858\n",
      "421 0.004212355241179466\n",
      "422 0.0040805758908391\n",
      "423 0.003949015401303768\n",
      "424 0.0038274277467280626\n",
      "425 0.0037074682768434286\n",
      "426 0.0035892988089472055\n",
      "427 0.0034768953919410706\n",
      "428 0.003373854560777545\n",
      "429 0.0032689273357391357\n",
      "430 0.0031715421937406063\n",
      "431 0.003073285100981593\n",
      "432 0.002979917684569955\n",
      "433 0.0028910653200000525\n",
      "434 0.0028023086488246918\n",
      "435 0.00271611288189888\n",
      "436 0.0026348119135946035\n",
      "437 0.0025562995579093695\n",
      "438 0.002479115268215537\n",
      "439 0.0024087042547762394\n",
      "440 0.002337120706215501\n",
      "441 0.002266903640702367\n",
      "442 0.002200476126745343\n",
      "443 0.0021353433839976788\n",
      "444 0.0020733990240842104\n",
      "445 0.0020121156703680754\n",
      "446 0.001954497303813696\n",
      "447 0.001898659160360694\n",
      "448 0.0018447168404236436\n",
      "449 0.001792985713109374\n",
      "450 0.0017431658925488591\n",
      "451 0.0016946321120485663\n",
      "452 0.0016482042847201228\n",
      "453 0.0016030725091695786\n",
      "454 0.0015554199926555157\n",
      "455 0.0015141010517254472\n",
      "456 0.001471074065193534\n",
      "457 0.0014330840203911066\n",
      "458 0.0013938859337940812\n",
      "459 0.0013559535145759583\n",
      "460 0.0013189306482672691\n",
      "461 0.0012831955682486296\n",
      "462 0.0012495596893131733\n",
      "463 0.0012152912095189095\n",
      "464 0.0011828874703496695\n",
      "465 0.0011520328698679805\n",
      "466 0.0011204498587176204\n",
      "467 0.0010928782867267728\n",
      "468 0.0010657530510798097\n",
      "469 0.0010379336308687925\n",
      "470 0.0010118429781869054\n",
      "471 0.0009861286962404847\n",
      "472 0.0009616658207960427\n",
      "473 0.0009376028901897371\n",
      "474 0.00091555219842121\n",
      "475 0.000891436415258795\n",
      "476 0.0008690918912179768\n",
      "477 0.0008466770523227751\n",
      "478 0.0008258665911853313\n",
      "479 0.0008074439829215407\n",
      "480 0.0007873258437030017\n",
      "481 0.0007678285473957658\n",
      "482 0.000750401639379561\n",
      "483 0.0007328132051043212\n",
      "484 0.0007142858230508864\n",
      "485 0.0006976916338317096\n",
      "486 0.0006827875622548163\n",
      "487 0.0006655646720901132\n",
      "488 0.0006494952831417322\n",
      "489 0.0006341986590996385\n",
      "490 0.0006197911570779979\n",
      "491 0.000605202978476882\n",
      "492 0.0005915008368901908\n",
      "493 0.000579186191316694\n",
      "494 0.0005660298629663885\n",
      "495 0.0005516864475794137\n",
      "496 0.0005397910717874765\n",
      "497 0.0005276924930512905\n",
      "498 0.0005163701716810465\n",
      "499 0.0005054586217738688\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(500):\n",
    "    # Forward pass: \n",
    "    # compute predicted y using operations on Tensors; \n",
    "    # these are exactly the same operations that\n",
    "    # we used to compute the forward pass using Tensors, \n",
    "    # but we do not need to keep references to intermediate values \n",
    "    # since we are not implementing the backward pass by hand.\n",
    "    # that is, we don't calculate grad_relu by hand\n",
    "    # \n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the a scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    \n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. \n",
    "    # Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    \n",
    "    # An alternative way is to operate on weight.data and weight.grad.data.\n",
    "    # Recall that tensor.data gives a tensor that shares the storage with\n",
    "    # tensor, but doesn't track history.\n",
    "    # You can also use torch.optim.SGD to achieve this.\n",
    "    with torch.no_grad():\n",
    "        # Wrap in torch.no_grad()\n",
    "        # because weights have requires_grad=True, \n",
    "        # but we don't need to track this\n",
    "        # in autograd.\n",
    "    \n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        # how to understand this???\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
